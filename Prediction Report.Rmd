---
title: "Prediction Report"
author: "Lei Cheng"
date: "January 29, 2019"
output: 
  html_document:
    keep_md: true
---
#Summary:

Six participants were asked to perform one set of 10 repetitions of barbell lifts correctly and incorrectly in 5 different ways. Data were collected from the accelerometers on the belt, forearm, arm, and dumbell. In this project, we will use the data to predict the **manner** in which participants did the exercise. 

```{r message=F}
library(tidyverse)
library(caret)
library(parallel)
library(doParallel)
library(randomForest)
library(rattle)
```

##Loading Data
```{r cache=T, message=F,warning=F}
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",destfile = "pml-training.csv")
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",destfile = "pml-testing.csv")
training<-read_csv("pml-training.csv")
testing<-read_csv("pml-testing.csv")
```

##Data Cleaning and Splitting

The data set includes summary statistics with variable names prefixed with "kurtosis_", "skewness_", "max_", "min_", "amplitude_", "var_", "avg_", and "stddev_". These columns contain very high proportions of missing values. I dropped the columns with mostly NA's and used only the raw sensor data. I also dropped the first few columns that contain information on the participants and the time windows. To conduct cross validation, I splitted the *pml-training.csv* file into the training set and the validation set.

```{r}
training<-training%>%select_if(~!any(is.na(.)))
testing<-testing%>%select_if(~!any(is.na(.)))

training<-select(training,roll_belt:classe)
testing<-select(testing,roll_belt:magnet_forearm_z)

set.seed(12345)
inTrain<-createDataPartition(y=training$classe,p=0.7,list=F)
training<-training[inTrain,]
validation<-training[-inTrain,]
results<-validation$classe
validation<-validation%>%select(-classe)
```

##Configure Parallel Processing and *trainControl* Object

Parallel processing was configured on the computer in order to improve the performance of random forest. I also chose the k-fold cross validation with k=5 for the resampling method in the *trainControl* object.
```{r}
cluster <- makeCluster(detectCores() - 1) 
registerDoParallel(cluster)
fitControl <- trainControl(method = "cv", number = 5,
                           allowParallel = TRUE)
```

##Model Training and Cross Validation
I used four different algorithms (trees, boosting, bagging and random forest) and compared their performance. Random forest and bagging yield the optimal accuracy of over 99% in the prediction on the validation set. I expect the out of sample error rate to be above 80%.

###Trees
Prediction with classification trees only yields an accuracy of around 50%, which means this is not a good model for this problem.
```{r cache=T}
fit_trees<-train(classe~.,method="rpart",data = training)
fit_trees
fancyRpartPlot(fit_trees$finalModel)

pred<-predict(fit_trees,validation)
confMat<-confusionMatrix(pred,as.factor(results))
confMat$overall[1]
confMat$table
```

###Boosting
Boosting yields quite an improvement from classification trees. With a tree depth of 3 and 150 iterations, we can get an accuracy of over 97%. 
```{r cache=T}
fit_boosting<-train(classe~.,method="gbm", data = training, verbose=F)
fit_boosting
plot(fit_boosting)

pred<-predict(fit_boosting,validation)
confMat<-confusionMatrix(pred,as.factor(results))
confMat$overall[1]
confMat$table
```


###Bagging
Bagging classification trees with 25 bootstrap replications yields an accuracy of over 99%.  
```{r}
fit_bag <- train(classe~., method="treebag",data=training,trControl = fitControl)
fit_bag

pred<-predict(fit_bag,validation)
confMat<-confusionMatrix(pred,as.factor(results))
confMat$overall[1]
confMat$table 
```

###Random Forest
Random Forest yields an optimal accuracy of over 99% with the number of variables at each split mtry=2 and 500 trees.
```{r cache=T}
set.seed(12345)
fit_rf <- train(classe~., method="rf",data=training,trControl = fitControl)
fit_rf
fit_rf$finalModel

plot(fit_rf)
plot(fit_rf$finalModel)

pred<-predict(fit_rf,validation)
confMat<-confusionMatrix(pred,as.factor(results))
confMat$overall[1]
confMat$table
```

##De-register parallel processing cluster
```{r}
stopCluster(cluster)
registerDoSEQ()
```

##Conclusion and Prediction
Upon comparing the accuracy of the four models, I decided that both bagging and Random Forest would be a good model for this problem. When tested on the test set, they yield the same results in the predictions.
```{r}
testPredRF<-predict(fit_rf,testing)
testPredRF

testPredBag<-predict(fit_bag,testing)
testPredBag
```

